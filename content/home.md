# hello, friend :)
My name is sushi! I'm an automation researcher and all around curious cat. Welcome to my portal! In [resources](# "data-page=resources") you can find a bunch of resources for the topics I've been curious about lately, and [blurbs](# "data-page=blurbs") will contain miniature write-ups on interesting tid bits I find during my studies! Find links to my socials on the bottom. I hope you have a pleasant day! ❤️

### OSS Projects

Some open source stuff I've contributed to includes:

- [arbiter](https://github.com/anthias-labs/arbiter): Agent-based simulation framework for Ethereum execution environments.
- [type_sheaf](https://github.com/Autoparallel/type_sheaf): Sheaves of types in Rust. WIP.

It's my new years resolution for 2025 to make more of an effort this year to do open source work!!

### Reading List

I figure it'll be nice to start keeping track of a proper reading list instead of just relying on bookmarks; so why not share where I'm at in my studies with y'all!

I want to read:
 - [Using Algebraic Geometry](https://eclass.uoa.gr/modules/document/file.php/D231/Papers/Cox-UsingAlgebraicGeometry.pdf)
 - [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
 - [Topos and Stacks of Deep Neural Networks](https://arxiv.org/abs/2106.14587)
 - [On the Ability of Deep Networks to Learn Symmetries from Data: A Neural Kernel Theory](https://arxiv.org/html/2412.11521v1)
 - [Improving Convergence and Generalization using Parameter Symmetries](https://openreview.net/pdf?id=L0r0GphlIL)
 - [Pruning's Effect on Generalization Through the Lens of Training and Regularization](https://arxiv.org/abs/2210.13738)
 - [Rigging the Lottery: Making All Tickets Winners](https://arxiv.org/abs/1911.11134)

Currently, I'm reading:

 - [On Symmetries of Deep Learning Models and their Internal Representations](https://arxiv.org/abs/2205.14258)
 - [Incorporating Symmetry into Deep Dynamics Models for Improved Generalization](https://arxiv.org/abs/2002.03061)
 - [Symmetry Teleportation for Accelerated Optimization](https://arxiv.org/abs/2205.10637)
 - [Grobner Bases: A Computaional Approach to Commutative Algebra](https://link.springer.com/book/10.1007/978-1-4612-0913-3)

On the backburner I have:

 - [Artin's deformation theory notes](https://www.maths.ed.ac.uk/~ssierra/artin_notes_deformationthy.pdf)
  - [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635)

Some stuff I've read and want to remember:

 - [Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs](https://arxiv.org/abs/2202.04579)
 - [Simplicial Attention Networks](https://arxiv.org/abs/2204.09455)
 - [Sheaf Neural Networks with Connection Laplacians](https://proceedings.mlr.press/v196/barbero22a/barbero22a.pdf)
 - [Topological Deep Learning: Classification Neural Networks](https://arxiv.org/abs/2102.08354)
 - [Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration](https://arxiv.org/abs/2202.04628)
 - [When is Partially Observable Reinforcement Learning Not Scary?](https://arxiv.org/abs/2204.08967)
 - [The Definitive Guide to Policy Gradients in Deep Reinforcement
Learning](https://arxiv.org/abs/2401.13662)
 - [Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl](https://arxiv.org/abs/2305.01582)
 - [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
 - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
 - [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)
 - [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
 - [Model Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
